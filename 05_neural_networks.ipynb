{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c4deb7-0447-4b18-b427-7ef66824ecb1",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fb861a-00e5-4cfc-a6cd-16da1a025f95",
   "metadata": {},
   "source": [
    "Neural Networks are powerful learning algorithm inspired by how the brain works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467e2477-9e46-4f8d-89a2-0aec56421ce7",
   "metadata": {},
   "source": [
    "Before diving into the details let's talk a little bit about how Neural Networks works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5892f4f6-ab88-4c0b-b226-99c902356988",
   "metadata": {},
   "source": [
    "Recall the house price prediciton from first session, which we have a vector of house features like size, number of bedrooms, etc. and we want to estimate the house price by employing a machine learning techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89139461-1741-4d81-a70a-813834a1f9ee",
   "metadata": {},
   "source": [
    "$$\n",
    "x = \\begin{bmatrix}size \\\\ \\# of bedrooms \\\\zip code \\\\ wealth\\end{bmatrix} \\rightarrow y = Price\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a19db-2dd2-4992-98a6-df793c1ac182",
   "metadata": {},
   "source": [
    "However, as we mentioned in earlier sessions these raw features may not be appropriate for machine learning algorithms. So we need to apply some feature engineering techniques to reduce, transform an prepare the features. In other words we transform the features to another space that is more informative for the desired task. For example, in classification task the discrimination of feature space is plays a key role in classifiers performance. So if the initial feature space does not have this attribute in feature engineering phase we transform it to a more dircriminative space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ea1bea-d899-4ba5-ae27-ace39962d6c4",
   "metadata": {},
   "source": [
    "**Neural Networks** apply some **learnable** transformations over and over to reach a feature space (latent space) which is best for the desired task. In other words they apply feature engineering phase automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3abd42-3a20-4b66-960d-2b40ec608763",
   "metadata": {},
   "source": [
    "<img src=\"images/neural_networks.png\" alt=\"neural networks\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4fdbd1-a27d-48fb-a932-f539bb3e0ac4",
   "metadata": {},
   "source": [
    "https://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd46de9-531b-4a54-b570-dc5dc13c45e2",
   "metadata": {},
   "source": [
    "## A Single Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e453a8-07d7-44d8-a332-e147eb79f84e",
   "metadata": {},
   "source": [
    "Recall from logistic regression session the below figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ec532-233f-40d6-aa9b-0ea7e4a45109",
   "metadata": {},
   "source": [
    "<img src=\"images/nn_single_neuron.png\" alt=\"single neuron\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980a08a7-9216-43ee-916d-c8facb75ffc3",
   "metadata": {},
   "source": [
    "this is exactly the structre of each neuron in neural network structure. It means each neuron in a neural network takes the outputs of previous layer, apply a linear function then feed to a nonlinaer function. The feature transformation in neural networks is constructed by identical simple linear-nonlinear building blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8923567-c328-4cc8-bc0a-ef7d366427b1",
   "metadata": {},
   "source": [
    "<img src=\"images/nn_multi_neuron.png\" alt=\"single neuron\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cc466a-d4fb-4c9e-b396-3e9709fd617a",
   "metadata": {},
   "source": [
    "## Common Non Linearities (Activation Functions) in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c101e09-5469-4c5a-9de5-0effc31f9b18",
   "metadata": {},
   "source": [
    "### 1. Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2949f64-affa-4248-8b28-e6093d9385d5",
   "metadata": {},
   "source": [
    "<img src=\"images/sigmoid.png\" alt=\"sigmoid\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3abd4a-6031-4d25-be70-dd80eb137e6e",
   "metadata": {},
   "source": [
    "If we look carefully at the graph towards the ends of the function, y values react very little to the changes in x. Let’s think about what kind of problem it is! The derivative values in these regions are very small and converge to 0. This is called the vanishing gradient and the learning is minimal. if 0, not any learning! When slow learning occurs, the optimization algorithm that minimizes error can be attached to local minimum values and cannot get maximum performance from the artificial neural network model. So let’s continue our search for an alternative activation function!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293432e-8939-49da-8dd6-92983b3bb2dc",
   "metadata": {},
   "source": [
    "### 2. Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7f86bf-1867-403a-8156-52089364f011",
   "metadata": {},
   "source": [
    "<img src=\"images/tanh.png\" alt=\"tanh\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541ef7ec-6fb2-4a35-a071-aab6b928c3bf",
   "metadata": {},
   "source": [
    "The advantage over the sigmoid function is that its derivative is more steep, which means it can get more value. This means that it will be more efficient because it has a wider range for faster learning and grading. But again, the problem of gradients at the ends of the function continues. Although we have a very common activation function, we will continue our search to find the best one!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a071d4-0288-4bef-8160-75c709c550b0",
   "metadata": {},
   "source": [
    "### 3. RELU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30967ee-6677-4620-b9b1-9728009e50c8",
   "metadata": {},
   "source": [
    "<img src=\"images/relu.png\" alt=\"relu\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c4355-d4eb-43ba-80c2-3343d7c5460e",
   "metadata": {},
   "source": [
    "Let’s imagine a large neural network with too many neurons. Sigmoid and hyperbolic tangent caused almost all neurons to be activated in the same way. This means that the activation is very intensive. Some of the neurons in the network are active, and activation is infrequent, so we want an efficient computational load. We get it with ReLU. Having a value of 0 on the negative axis means that the network will run faster. The fact that the calculation load is less than the sigmoid and hyperbolic tangent functions has led to a higher preference for multi-layer networks. Super! But even ReLU isn’t exactly great, why? Because of this zero value region that gives us the speed of the process! So the learning is not happening in that area. Then you need to find a new activation function with a trick."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b6610-c1f0-4dfa-8864-3da3aa30cc3b",
   "metadata": {},
   "source": [
    "### 4. Lealy RELU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba9ecb-16c2-459a-9856-71314bfc99d0",
   "metadata": {},
   "source": [
    "<img src=\"images/lealy_relu.png\" alt=\"lealy relu\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7cc6b9-abfd-426b-a41f-9b6357c7aa1a",
   "metadata": {},
   "source": [
    "more on this topic: https://towardsdatascience.com/comparison-of-activation-functions-for-deep-neural-networks-706ac4284c8a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bf6922-79b2-455a-989a-a94b6d0941d8",
   "metadata": {},
   "source": [
    "## Mathematical Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a330a9f4-5fd2-4759-a650-1d01f2eab350",
   "metadata": {},
   "source": [
    "<img src=\"images/layer_with_notation.png\" alt=\"with notation\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b62208-e265-4db1-9e5b-0a22effe0408",
   "metadata": {},
   "source": [
    "$$\n",
    "z_1 = w_1^1x_1 + w_2^1x_2 + w_3^1x_3 + b^1 \\\\\n",
    "a_1 = \\sigma(z_1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09faca6f-cf2f-44b0-9493-3bf1411ae48b",
   "metadata": {},
   "source": [
    "$$\n",
    "z_2 = w_1^2x_1 + w_2^2x_2 + w_3^2x_3 + b^2 \\\\\n",
    "a_2 = \\sigma(z_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e408c51-4f4a-4c7b-92cc-dd1c1d1cdc03",
   "metadata": {},
   "source": [
    "### Matrix Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a71bf5-a766-48f8-93ae-1e4e9d8aa855",
   "metadata": {},
   "source": [
    "$$\n",
    "z_1 = w_1^1x_1 + w_2^1x_2 + w_3^1x_3 + b^1 \\\\\n",
    "z_2 = w_1^2x_1 + w_2^2x_2 + w_3^2x_3 + b^2 \\\\\n",
    "z_3 = w_1^3x_1 + w_2^3x_2 + w_3^3x_3 + b^3 \\\\\n",
    "z_4 = w_1^4x_1 + w_2^4x_2 + w_3^4x_3 + b^4 \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94883c-e700-4917-a552-a106b43071ab",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}z_1 \\\\ z_2 \\\\z_3 \\\\ z_4\\end{bmatrix} = \\begin{bmatrix}w_1^1 & w_2^1 & w_3^1 \\\\ w_1^2 & w_2^2 & w_3^2 \\\\ w_1^3 & w_2^3 & w_3^3 \\\\ w_1^4 & w_2^4 & w_3^4\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2 \\\\x_3\\end{bmatrix} + \\begin{bmatrix}b^1 \\\\ b^2 \\\\b^3 \\\\ b^4\\end{bmatrix} \\\\\n",
    "\\textbf{z} = \\textbf{W}\\textbf{x} + \\textbf{b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bace9d4-e097-497b-9e7a-e12a6b0d9d39",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}a_1 \\\\ a_2 \\\\a_3 \\\\ a_4\\end{bmatrix} = \\sigma(\\begin{bmatrix}z_1 \\\\ z_2 \\\\z_3 \\\\ z_4\\end{bmatrix}) \\\\\n",
    "\\textbf{a} = \\sigma(\\textbf{z})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e012e-d078-478f-a033-4e445d5d4a18",
   "metadata": {},
   "source": [
    "for each layer of neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f05fc61-c1d4-41bd-b508-5181a0869f7f",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{z}^{[l]} = \\textbf{W}^{[l]}\\textbf{a}^{[l-1]} + \\textbf{b}^{[l]} \\\\\n",
    "\\textbf{a}^{[l]} = \\sigma(\\textbf{z}^{[l]})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f23cb17-004a-44ea-b212-79dcb912811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae7c63c6-d7c2-4c11-a269-e4329c3526e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5240171-b1f9-4912-b464-67d17ef1b1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(digits.images))\n",
    "print(type(digits.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa2d48a-c607-4f9f-87f2-bfb125994a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "156a32aa-ec5e-47e4-9b2b-a58d767d8394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],\n",
       "       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],\n",
       "       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],\n",
       "       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],\n",
       "       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],\n",
       "       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],\n",
       "       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],\n",
       "       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6bd626d-a975-4759-a828-8f383012e1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(digits.images[0],cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba815bf6-7203-411c-9aa9-c9ff9acd4f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797,)\n",
      "[0 1 2 ... 8 9 8]\n"
     ]
    }
   ],
   "source": [
    "print(digits.target.shape)\n",
    "print(digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a2af19d-68be-4f4d-87b3-d46c87617742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi(i):\n",
    "    '''Plots 16 digits, starting with digit i'''\n",
    "    nplots = 16\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    for j in range(nplots):\n",
    "        plt.subplot(4,4,j+1)\n",
    "        plt.imshow(digits.images[i+j], cmap='binary')\n",
    "        plt.title(digits.target[i+j])\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ee3527-0896-4795-a41a-ecd93c3c5b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_multi(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ffa0135-ad08-4ba6-b8a8-ec4960a68a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = digits.target\n",
    "x = digits.images.reshape((len(digits.images), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "428a1af9-10e8-4ac5-96ac-264ef23d11b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d20fa04f-53b1-4bff-9afc-7e3f0d50f8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224.625"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1797/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "242b5d40-f5f3-4c63-b34b-9ee32407f8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c1f94d3-c37c-48c1-9422-b47b63c18ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68959d-746e-444e-b81a-59f2420b9827",
   "metadata": {},
   "source": [
    "<img src=\"images/multi_class_mlp.png\" alt=\"neural networks\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "830f4f24-b3b9-4226-86de-c32ed7626af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(15,), activation='logistic', alpha=1e-4,\n",
    "                    solver='sgd', tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee91375b-370d-4054-8b65-ad6460ca6a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.17958699\n",
      "Iteration 2, loss = 1.77146055\n",
      "Iteration 3, loss = 1.37439429\n",
      "Iteration 4, loss = 1.02004886\n",
      "Iteration 5, loss = 0.75415292\n",
      "Iteration 6, loss = 0.57924176\n",
      "Iteration 7, loss = 0.45963913\n",
      "Iteration 8, loss = 0.37982746\n",
      "Iteration 9, loss = 0.32033652\n",
      "Iteration 10, loss = 0.28769840\n",
      "Iteration 11, loss = 0.24265526\n",
      "Iteration 12, loss = 0.21130886\n",
      "Iteration 13, loss = 0.19594798\n",
      "Iteration 14, loss = 0.17002398\n",
      "Iteration 15, loss = 0.15547854\n",
      "Iteration 16, loss = 0.14189883\n",
      "Iteration 17, loss = 0.13746330\n",
      "Iteration 18, loss = 0.12195878\n",
      "Iteration 19, loss = 0.12207813\n",
      "Iteration 20, loss = 0.11055786\n",
      "Iteration 21, loss = 0.10030376\n",
      "Iteration 22, loss = 0.09741844\n",
      "Iteration 23, loss = 0.08985571\n",
      "Iteration 24, loss = 0.08772165\n",
      "Iteration 25, loss = 0.08282575\n",
      "Iteration 26, loss = 0.08106593\n",
      "Iteration 27, loss = 0.08030491\n",
      "Iteration 28, loss = 0.07282371\n",
      "Iteration 29, loss = 0.07003939\n",
      "Iteration 30, loss = 0.06810777\n",
      "Iteration 31, loss = 0.06578545\n",
      "Iteration 32, loss = 0.06349730\n",
      "Iteration 33, loss = 0.06132775\n",
      "Iteration 34, loss = 0.05884412\n",
      "Iteration 35, loss = 0.06035070\n",
      "Iteration 36, loss = 0.05628312\n",
      "Iteration 37, loss = 0.05581657\n",
      "Iteration 38, loss = 0.05280723\n",
      "Iteration 39, loss = 0.05121309\n",
      "Iteration 40, loss = 0.04996599\n",
      "Iteration 41, loss = 0.04956204\n",
      "Iteration 42, loss = 0.04750926\n",
      "Iteration 43, loss = 0.04654444\n",
      "Iteration 44, loss = 0.04558040\n",
      "Iteration 45, loss = 0.04349860\n",
      "Iteration 46, loss = 0.04319277\n",
      "Iteration 47, loss = 0.04193736\n",
      "Iteration 48, loss = 0.04212263\n",
      "Iteration 49, loss = 0.04047623\n",
      "Iteration 50, loss = 0.03938713\n",
      "Iteration 51, loss = 0.03872691\n",
      "Iteration 52, loss = 0.03842558\n",
      "Iteration 53, loss = 0.03759840\n",
      "Iteration 54, loss = 0.03660393\n",
      "Iteration 55, loss = 0.03571880\n",
      "Iteration 56, loss = 0.03529850\n",
      "Iteration 57, loss = 0.03489943\n",
      "Iteration 58, loss = 0.03402574\n",
      "Iteration 59, loss = 0.03368057\n",
      "Iteration 60, loss = 0.03365672\n",
      "Iteration 61, loss = 0.03256263\n",
      "Iteration 62, loss = 0.03186807\n",
      "Iteration 63, loss = 0.03195238\n",
      "Iteration 64, loss = 0.03148349\n",
      "Iteration 65, loss = 0.03070676\n",
      "Iteration 66, loss = 0.03041987\n",
      "Iteration 67, loss = 0.02984827\n",
      "Iteration 68, loss = 0.02932446\n",
      "Iteration 69, loss = 0.02933910\n",
      "Iteration 70, loss = 0.02877839\n",
      "Iteration 71, loss = 0.02819168\n",
      "Iteration 72, loss = 0.02779422\n",
      "Iteration 73, loss = 0.02772134\n",
      "Iteration 74, loss = 0.02732713\n",
      "Iteration 75, loss = 0.02703032\n",
      "Iteration 76, loss = 0.02660855\n",
      "Iteration 77, loss = 0.02619855\n",
      "Iteration 78, loss = 0.02579309\n",
      "Iteration 79, loss = 0.02565044\n",
      "Iteration 80, loss = 0.02539973\n",
      "Iteration 81, loss = 0.02494927\n",
      "Iteration 82, loss = 0.02475633\n",
      "Iteration 83, loss = 0.02450480\n",
      "Iteration 84, loss = 0.02417021\n",
      "Iteration 85, loss = 0.02395803\n",
      "Iteration 86, loss = 0.02389122\n",
      "Iteration 87, loss = 0.02333162\n",
      "Iteration 88, loss = 0.02322169\n",
      "Iteration 89, loss = 0.02285889\n",
      "Iteration 90, loss = 0.02249322\n",
      "Iteration 91, loss = 0.02208255\n",
      "Iteration 92, loss = 0.02189783\n",
      "Iteration 93, loss = 0.02186254\n",
      "Iteration 94, loss = 0.02136466\n",
      "Iteration 95, loss = 0.02134573\n",
      "Iteration 96, loss = 0.02101282\n",
      "Iteration 97, loss = 0.02079735\n",
      "Iteration 98, loss = 0.02058525\n",
      "Iteration 99, loss = 0.02035023\n",
      "Iteration 100, loss = 0.02015184\n",
      "Iteration 101, loss = 0.01993088\n",
      "Iteration 102, loss = 0.01971475\n",
      "Iteration 103, loss = 0.01948070\n",
      "Iteration 104, loss = 0.01938396\n",
      "Iteration 105, loss = 0.01913777\n",
      "Iteration 106, loss = 0.01889929\n",
      "Iteration 107, loss = 0.01870131\n",
      "Iteration 108, loss = 0.01863996\n",
      "Iteration 109, loss = 0.01846352\n",
      "Iteration 110, loss = 0.01822342\n",
      "Iteration 111, loss = 0.01799772\n",
      "Iteration 112, loss = 0.01791524\n",
      "Iteration 113, loss = 0.01780220\n",
      "Iteration 114, loss = 0.01766386\n",
      "Iteration 115, loss = 0.01751565\n",
      "Iteration 116, loss = 0.01743794\n",
      "Iteration 117, loss = 0.01718054\n",
      "Iteration 118, loss = 0.01701334\n",
      "Iteration 119, loss = 0.01688861\n",
      "Iteration 120, loss = 0.01682901\n",
      "Iteration 121, loss = 0.01655162\n",
      "Iteration 122, loss = 0.01657236\n",
      "Iteration 123, loss = 0.01637595\n",
      "Iteration 124, loss = 0.01619220\n",
      "Iteration 125, loss = 0.01622168\n",
      "Iteration 126, loss = 0.01598083\n",
      "Iteration 127, loss = 0.01592057\n",
      "Iteration 128, loss = 0.01575651\n",
      "Iteration 129, loss = 0.01567600\n",
      "Iteration 130, loss = 0.01556187\n",
      "Iteration 131, loss = 0.01538553\n",
      "Iteration 132, loss = 0.01539924\n",
      "Iteration 133, loss = 0.01524961\n",
      "Iteration 134, loss = 0.01514073\n",
      "Iteration 135, loss = 0.01509505\n",
      "Iteration 136, loss = 0.01496522\n",
      "Iteration 137, loss = 0.01483125\n",
      "Iteration 138, loss = 0.01470602\n",
      "Iteration 139, loss = 0.01465325\n",
      "Iteration 140, loss = 0.01455429\n",
      "Iteration 141, loss = 0.01441655\n",
      "Iteration 142, loss = 0.01431597\n",
      "Iteration 143, loss = 0.01420626\n",
      "Iteration 144, loss = 0.01417681\n",
      "Iteration 145, loss = 0.01406745\n",
      "Iteration 146, loss = 0.01399770\n",
      "Iteration 147, loss = 0.01388374\n",
      "Iteration 148, loss = 0.01384564\n",
      "Iteration 149, loss = 0.01371295\n",
      "Iteration 150, loss = 0.01362736\n",
      "Iteration 151, loss = 0.01352135\n",
      "Iteration 152, loss = 0.01348404\n",
      "Iteration 153, loss = 0.01337620\n",
      "Iteration 154, loss = 0.01326692\n",
      "Iteration 155, loss = 0.01324553\n",
      "Iteration 156, loss = 0.01312977\n",
      "Iteration 157, loss = 0.01303642\n",
      "Iteration 158, loss = 0.01303624\n",
      "Iteration 159, loss = 0.01288448\n",
      "Iteration 160, loss = 0.01279911\n",
      "Iteration 161, loss = 0.01280795\n",
      "Iteration 162, loss = 0.01269021\n",
      "Iteration 163, loss = 0.01259203\n",
      "Iteration 164, loss = 0.01252728\n",
      "Iteration 165, loss = 0.01253539\n",
      "Iteration 166, loss = 0.01241974\n",
      "Iteration 167, loss = 0.01232052\n",
      "Iteration 168, loss = 0.01224880\n",
      "Iteration 169, loss = 0.01222868\n",
      "Iteration 170, loss = 0.01217481\n",
      "Iteration 171, loss = 0.01215146\n",
      "Iteration 172, loss = 0.01207105\n",
      "Iteration 173, loss = 0.01200643\n",
      "Iteration 174, loss = 0.01194206\n",
      "Iteration 175, loss = 0.01183057\n",
      "Iteration 176, loss = 0.01179649\n",
      "Iteration 177, loss = 0.01172683\n",
      "Iteration 178, loss = 0.01164049\n",
      "Iteration 179, loss = 0.01166895\n",
      "Iteration 180, loss = 0.01157569\n",
      "Iteration 181, loss = 0.01152695\n",
      "Iteration 182, loss = 0.01142461\n",
      "Iteration 183, loss = 0.01138887\n",
      "Iteration 184, loss = 0.01134329\n",
      "Iteration 185, loss = 0.01130536\n",
      "Iteration 186, loss = 0.01120358\n",
      "Iteration 187, loss = 0.01115122\n",
      "Iteration 188, loss = 0.01111604\n",
      "Iteration 189, loss = 0.01101982\n",
      "Iteration 190, loss = 0.01100433\n",
      "Iteration 191, loss = 0.01093434\n",
      "Iteration 192, loss = 0.01094593\n",
      "Iteration 193, loss = 0.01084592\n",
      "Iteration 194, loss = 0.01079615\n",
      "Iteration 195, loss = 0.01071193\n",
      "Iteration 196, loss = 0.01067340\n",
      "Iteration 197, loss = 0.01064449\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', hidden_layer_sizes=(15,),\n",
       "              learning_rate_init=0.1, random_state=1, solver='sgd',\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88b9ff35-d575-4170-b064-ff9d274670a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533333333333334"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c4429e-c1ac-49c5-8a20-18454900c2c1",
   "metadata": {},
   "source": [
    "## Training Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b465077c-d0dd-41c8-be80-a724e9ddc3b5",
   "metadata": {},
   "source": [
    "recall from previous sessions, most of the machine learning training algorithms (neural networks in this case) consist of these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69519009-349c-496b-afe3-9845d3b2c964",
   "metadata": {},
   "source": [
    "1. Choose a  neural network structure like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6361f-7e5a-4134-a4d5-d74c75b23a2b",
   "metadata": {},
   "source": [
    "<img src=\"images/neural_networks.png\" alt=\"neural networks\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9900c394-02ad-4494-b7fe-eb4ac4b4d1b7",
   "metadata": {},
   "source": [
    "2. Initialize weights $W$ and biases $b$ of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547d721-a31d-496f-9c26-356202bcba4a",
   "metadata": {},
   "source": [
    "3. Feed all datapoints to the network and compute the network output. (feed forward) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112a3013-7075-4099-ac69-c26bb87f3506",
   "metadata": {},
   "source": [
    "4. compute a loss function for all outputs:\n",
    "\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{i=1}^{N}L(y_{pred}^{(i)}, y_{true}^{(i)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a1a2d0-ba32-4423-af41-db52d0587815",
   "metadata": {},
   "source": [
    "5. Compute the gradient of loss function with respect to model weights and biases. (back propagation)\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W}=\\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\partial L^{(i)}}{\\partial W} \\\\\n",
    "\\frac{\\partial L}{\\partial b}=\\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\partial L^{(i)}}{\\partial b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7457bb6-7167-43d6-89c9-7ea297598606",
   "metadata": {},
   "source": [
    "6. Update weights and biases with gradient descent algorithm:\n",
    "$$\n",
    "W^{new} = W^{old} - lr \\times \\frac{\\partial L}{\\partial W^{old}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fbcc71-a8c2-4838-866a-ea73fc152a17",
   "metadata": {},
   "source": [
    "7. back to step 3 and iterate until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bfc338-23d3-4609-8a0b-dc82c697cbb7",
   "metadata": {},
   "source": [
    "Although this method can effectivly find the (local) minimum of cost function, it can be computationally expensive since for each step of optimization all of datapoints should be fed into the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e734fc2-2eb0-4c4f-8f05-808408cbe078",
   "metadata": {},
   "source": [
    "### Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ded0a31-c011-4891-9496-52a133599c5f",
   "metadata": {},
   "source": [
    "<img src=\"images/minibatch.png\" alt=\"neural networks\" width=\"350\" height=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dfe2c3-c157-400b-b21d-c58fc53e3358",
   "metadata": {},
   "source": [
    "In this approach first we divide the dataset into number of batches with an equal number of data in each batch (batch size). Then in each optimization step feed just one batch to network and update weights and biases, then feed next batch, and so on. When all batches have been fed to the network we say one **epoch** have been completed. We can train several epochs until the optimization technique converge to a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c5b4ca-ad17-4285-a9a7-9a8badadd37a",
   "metadata": {},
   "source": [
    "The special case of mini-batch gradient descent is called **Stochastic Gradient Descent (SGD)** in which the batch size is equal to one. In other word each optimization step is performed by just one data point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e6d00-0408-45fa-931f-1a0126cec7c7",
   "metadata": {},
   "source": [
    "Since the mini-batch gradient descent is performed on a group of data points in each step and not all data points. the direction of weights update is a little bit different from the original gradient descent and have a little noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3675a6eb-9d43-4e48-afdc-ec0a8a657956",
   "metadata": {},
   "source": [
    "<img src=\"images/minibatch_vs_batch_gd.png\" alt=\"neural networks\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9a61c-57e7-47a3-9f2d-01a0423bc3cf",
   "metadata": {},
   "source": [
    "So there are number of optimization algorithms that solve this problem and act like a denoising filter for mini-batch gradient descent. Some of the famous optimization algorithms in Neural Networks are:\n",
    "1. RMSprop\n",
    "2. Adam\n",
    "3. AdaDelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886ab2fc-ce34-48f8-93b7-c5d77f37d34c",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543fc715-49b0-4c52-90d5-cde73fe42b2a",
   "metadata": {},
   "source": [
    "Backpropagation is the essence of neural network training. It is the method of computing the gradient of the weights of a neural network based on the error rate obtained in the iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c62c77-9367-4b78-ad7e-e669b3bf2695",
   "metadata": {},
   "source": [
    "Backpropagation in neural network is a short form for \"backward propagation of errors.\" It is a standard method of training artificial neural networks. This method helps calculate the gradient of a loss function with respect to all the weights in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2bfbc0-21f3-4246-978c-00357d19964b",
   "metadata": {},
   "source": [
    "The Back propagation algorithm in neural network computes the gradient of the loss function for a single weight by the **chain rule**. It efficiently computes one layer at a time, unlike a native direct computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7119b84e-7dfa-45ea-a598-a30c87f2c9a3",
   "metadata": {},
   "source": [
    "<img src=\"images/backpropagation.png\" alt=\"back propagation\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de2c3ab-8125-4645-aacb-3c884fbeebb5",
   "metadata": {},
   "source": [
    "For computing the gradient of each layer's weights we need the gradient of next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9856ef03-3b45-4594-9caf-de2aeda08a08",
   "metadata": {},
   "source": [
    "### Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f186ea-6a8b-41d9-86cf-fb7754f9772a",
   "metadata": {},
   "source": [
    "As we mentioned the gradient of each layer is a function of next layers. For example to computing the early layer's gradients in an n-hidden layer network, the gradient of n activation function is multiplied. \n",
    "\n",
    "when n hidden layers use an activation like the sigmoid function, n small derivatives are multiplied together. Thus, the gradient decreases exponentially as we propagate down to the initial layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015fed80-3d5c-494d-8807-f8053db1e2e3",
   "metadata": {},
   "source": [
    "A small gradient means that the weights and biases of the initial layers will not be updated effectively with each training session. Since these initial layers are often crucial to recognizing the core elements of the input data, it can lead to overall inaccuracy of the whole network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e920086-fef5-4f34-969e-24f3f6904dd5",
   "metadata": {},
   "source": [
    "For shallow network with only a few layers that use these activations, this isn’t a big problem. However, when more layers are used, it can cause the gradient to be too small for training to work effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde48b1-b3a4-472d-896a-7e5df0b87462",
   "metadata": {},
   "source": [
    "There are ways to detect whether your deep network is suffering from the vanishing gradient problem:\n",
    "1. The model will improve very slowly during the training phase and it is also possible that training stops very early, meaning that any further training does not improve the model.\n",
    "2. The weights closer to the output layer of the model would witness more of a change whereas the layers that occur closer to the input layer would not change much (if at all).\n",
    "3. Model weights shrink exponentially and become very small when training the model.\n",
    "4. The model weights become 0 in the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761e5e4d-309a-4f0d-814f-9a270b9cb687",
   "metadata": {},
   "source": [
    "The simplest solution is to use other activation functions, such as ReLU, which doesn’t cause a small derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7224e0a2-40ee-4579-953f-f97bf4b07b88",
   "metadata": {},
   "source": [
    "### Exploding Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3d54e0-404d-46d6-a58a-a435a015571a",
   "metadata": {},
   "source": [
    "This is exactly the opposite of vanishing gradient problem. In a network of n hidden layers, n derivatives will be multiplied together. If the derivatives are large then the gradient will increase exponentially as we propagate down the model until they eventually explode, and this is what we call the problem of exploding gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4186bbe0-7b26-495c-92a5-fe77e47bbaae",
   "metadata": {},
   "source": [
    "There are few subtle methods that you may use to determine whether your model is suffering from the problem of exploding gradients;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9238fc74-5cbb-44dc-aca0-860d6da7395b",
   "metadata": {},
   "source": [
    "1. The model is not learning much on the training data therefore resulting in a poor loss.\n",
    "2. The model will have large changes in loss on each update due to the models instability.\n",
    "3. The model will have large changes in loss on each update due to the models instability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e112b-4236-47fb-9e98-0ba950e9b22b",
   "metadata": {},
   "source": [
    "Checking for and limiting the size of the gradients whilst our model trains is a common solution for exploding gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49701de-3745-48e9-8112-318145099f37",
   "metadata": {},
   "source": [
    "A more careful initialization choice of the random initialization for your network tends to be a partial solution, since it does not solve the problem completely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
