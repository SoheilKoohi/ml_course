{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now we investigated problems such as image recognition, house price prediction, etc. In all of these problems for predicting the output of the system we should just look at a single data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example in image recognition problem the model just look at a single image and decide what is the probability of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the Fully Connected Neural Networks and CNNs can not handle **Sequential Data** in which data comes in a sequential manner and somehow the past data infulence the output of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Sequential Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Stock Market Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/stock_market.png\" alt=\"stock market\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sentiment_analysis.jpg\" alt=\"sentiment\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/machine_translation.png\" alt=\"classification\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/speech_recognition.jpeg\" alt=\"speech recognition\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Problem with Standard Neural Networks for Sequential Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two problems with standard neural networks:\n",
    "1. Inputs, outputs can be different lengths in different examples.\n",
    "    * The input sequence can be: **\"They don't know that we know they know we know.\"** or simply **\"I don't know\"**\n",
    "    * This can be solved for normal NNs by paddings with the maximum lengths but it's not a good solution.\n",
    "2. Fully connected  neural networks do not share learned features across different positions of text/sequence.\n",
    "    * Using a feature sharing like in CNNs can significantly reduce the number of parameters in your model. That's what we will do in RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Should the new architectures have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Have a memory mechanism to remember past inputs.\n",
    "2. Handle inputs and outputs with different length.\n",
    "3. Share features across the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Network remembers the past and it’s decisions are influenced by what it has learnt from the past. \n",
    "\n",
    "RNNs can take one or more input vectors and produce one or more output vectors and the output(s) are influenced not just by weights applied on inputs like a regular NN, but also by a “hidden” state vector representing the context based on prior input(s)/output(s). So, the same input could produce a different output depending on previous inputs in the series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rnn.png\" alt=\"rnn\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all of the weights and parameters are same in all stages and are shared with all positions of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rnn_cell.png\" alt=\"rnn cell\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Types of RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/types_of_rnns.jpg\" alt=\"types of rnns\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradient problem in RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems with vanilla RNNs that they run into vanishing gradient problem.\n",
    "Let's take an example. Suppose we are working on a NLP problem and there are two sequences that model tries to learn:\n",
    "1. \"The cat, which already ate ............................................................, was full\"\n",
    "2. \"The cats, which already ate ..........................................................., were full\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to learn here that \"was\" came with \"cat\" and that \"were\" came with \"cats\". The vanilla RNN is not very good at capturing very long-term dependencies like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have discussed in Deep neural networks, deeper networks are getting into the vanishing gradient problem. That also happens with RNNs with a long sequence size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/vanishing_gradient_rnn.png\" alt=\"vanishing gradient RNN\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conclusion is that RNNs aren't good in long-term dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU is an RNN type that can help solve the vanishing gradient problem and can remember the long-term dependencies.\n",
    "\n",
    "Each layer in GRUs has a new variable C which is the memory cell. It can tell to whether memorize something or not.\n",
    "\n",
    "Equations of the GRUs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gru_equation.png\" alt=\"gru equation\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update gate is between 0 and 1\n",
    "To understand GRUs imagine that the update gate is either 0 or 1 most of the time.\n",
    "So we update the memory cell based on the update cell and the previous cell.\n",
    "Lets take the cat sentence example and apply it to understand this equations:\n",
    "\n",
    "Sentence: \"The cat, which already ate ........................, was full\"\n",
    "\n",
    "We will suppose that U is 0 or 1 and is a bit that tells us if a singular word needs to be memorized.\n",
    "\n",
    "Splitting the words and get values of C and U at each place:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gru_table.png\" alt=\"gru table\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM is another type of RNN that can enable you to handle long-term dependencies. It's more powerful and general than GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the equations of an LSTM unit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lstm_equation.png\" alt=\"lstm equation\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lstm_cell.png\" alt=\"lstm cell\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't a universal superior between LSTM and it's variants. One of the advantages of GRU is that it's simpler and can be used to build much bigger network but the LSTM is more powerful and general.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the Name entity recognition task:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/name_entity_recognition.png\" alt=\"name entity recognition\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name Teddy cannot be learned from He and said, but can be learned from bears.\n",
    "\n",
    "BiRNNs fixes this issue.\n",
    "\n",
    "Here is BRNNs architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bidirectional_rnn.png\" alt=\"bidirectional rnn\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Part of the forward propagation goes from left to right, and part - from right to left. It learns from both sides.\n",
    "* To make predictions we use ŷ<t> by using the two activations that come from left and right.\n",
    "* The blocks here can be any RNN block including the basic RNNs, LSTMs, or GRUs.\n",
    "* For a lot of NLP or text processing problems, a BiRNN with LSTM appears to be commonly used.\n",
    "* The disadvantage of BiRNNs that you need the entire sequence before you can process it. For example, in live speech  recognition if you use BiRNNs you will need to wait for the person who speaks to stop to take the entire sequence and then make your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a lot of cases the standard one layer RNNs will solve your problem. But in some problems its useful to stack some RNN layers to make a deeper network.\n",
    "\n",
    "For example, a deep RNN with 3 layers would look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deep_rnn.png\" alt=\"deep rnn\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In fully connected deep nets, there could be 100 or even 200 layers. In deep RNNs stacking 3 layers is already considered deep and expensive to train.\n",
    "* In some cases you might see a fully connected network connected after recurrent cell. (like head layer in CNNs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Representation of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know the input of a neural network must be a vector. So for NLP tasks we need a way to represent words with numerical vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/word_embedding.png\" alt=\"word embedding\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Basic Ordinal Encoding of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what would be the most basic and simplest approach to this problem?\n",
    "\n",
    "Just assign a unique number to each word in the dictionary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|word    | number|\n",
    "| ---    |---|\n",
    "a       |  1\n",
    "able    |  2\n",
    "       ...\n",
    "book    |  325\n",
    "       ...\n",
    "paper   |  1024\n",
    "       ...\n",
    "zoology |  5067\n",
    "       ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a nice and simple representation, but it has one major drawback. In addition to the fact that these numbers don’t capture any meaning about the words, they, on the other hand, impose an implicit and unnecessary semantic ordering on the words. There is in fact no reason why paper should be assigned a greater number than book and a smaller number than zoology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. One-hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should consider other approaches that can overcome the inherent semantic ordering problem associated with numbering. This is where the vectors come into the picture. Let’s go back to our basic numbering approach and extend it to avoid the ordering issue. So, we will still assign a unique integer to each word, but instead of keeping the number in decimal form, we will encode the number into a binary vector (vector of 0s and 1s) of size |V| where V is the vocabulary of words in a given text. This vector will have 0s everywhere except at the index corresponding to the number we assigned to the word where we will put 1. Let’s look at a simple text example to illustrate this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I think therefore I am.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I&nbsp;&nbsp;think&nbsp;&nbsp;therefore&nbsp;&nbsp;am\\\n",
    "1&nbsp;&nbsp;2&nbsp;&nbsp;3&nbsp;&nbsp;4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/one_hot_words.png\" alt=\"one hot words\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is no notion of implicit ordering among these vectors, so we have achieved a more effective and yet simple representation of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this scheme is seldom used in practice due to the following main limitations of its own:\n",
    "\n",
    "  * While our toy example was short, in the real world you would need to process large amounts of text with big vocabularies. Since the dimension of our one-hot encoded word vector is directly proportional to the size of the vocabulary, we will end up with large sparse vectors where most of the entries are zeroes and this is computationally inefficient to deal with. Sparsity is also prone to cause overfitting.\n",
    "  * Each word is treated as an independent unit and there is no concept of a relationship between words such as similarity in this representation. In other words, this representation fails to capture the semantic meaning of words with respect to other words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will talk about other classical word representation approaches in final session. For now let's move on to more advanced and recent techniques called **Embedding**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In search of a meaningful representation for words, we have seen some interesting foundational approaches, yet we have been unable to escape the curse of high dimensionality and sparsity. Previous representations also seem to fail to encapsulate the true semantic meaning of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When are two words semantically similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we determine if two words are similar? For example, are the words man and woman similar? Certainly yes as both refer to human beings. So, in the context of human beings, these words are semantically similar. By this analogy, other words like king, queen, boy, and girl are also equally related in the same context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we switch the context, say, to be in terms of sex, then suddenly these terms do not seem to share the same degree of similarity. Clearly, in this context, man is more similar to boy than woman which is on its own more similar to girl than man.\n",
    "\n",
    "So we conclude that the context plays a pivotal role in determining semantic similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, following this reasoning, if we were somehow able to encode these contexts into a vector of numbers, then resulting vectors would be far more representative of their corresponding words than what we have seen so far. Let us illustrate it by an example. We are going to construct some fictitious vector for each of the words mentioned above (plus some additional words) based on the following contexts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Is it a human being?\n",
    "2. Is it male?\n",
    "3. Is it female?\n",
    "4. Is it a state ruler?\n",
    "5. Is it inanimate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/example_word_embedding.png\" alt=\"example of word embedding\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the values in each dimension represent the degree of our belief about the relatedness of the word to the context being considered. As you can see, words that are semantically similar in some context are also close to each other in the vector space on the dimension associated with the same context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of Embedding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Its dimension is no longer correlated with the size of our vocabulary, but rather with the number of contexts (features) which is usually significantly smaller and this means we can represent the words with low-dimensional, dense vectors that at the same time offers the crucial benefit of detecting semantic similarity.\n",
    "2. This representation also allows us to answer other interesting questions about the linguistic patterns. For example, we can ask man is to king as woman is to what? The linguistic answer is queen, and you can actually arrive at this answer by manipulating the vectors as following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/semantic_relation.png\" alt=\"semantic relation\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/word_vector_space.jpeg\" alt=\"word vector space\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also intuitively makes sense in the way that if you remove the manliness from the king, you should be left with the pure concept of royalty, and adding womanliness feature to this result should give you a queen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we have at our hand an ingenious approach that represents the words in a more semantically intuitive fashion with compact vectors (a.k.a word embeddings), the question becomes how do we actually come up with these vectors? We looked at a small sample corpus with a few contexts, but in real-world of humongous vocabularies, manually generating these vectors would be nearly impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in conclution word embedding is a **learned representation** for text where words that have the same meaning have a similar representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you implement an algorithm to learn a word embedding, what you end up learning is a embedding matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are using 10,000 words as our vocabulary (plus token).\\\n",
    "The algorithm should create a matrix E of the shape (300, 10000) in case we are extracting 300 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/embedding_matrix.png\" alt=\"embedding matrix\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the annotation $O_{idx}$ for any word that is represented with one-hot and $e_{idx}$ for embedding vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $O_{6257}$ is the one hot encoding of the word orange of shape (10000, 1), then\n",
    "$np.dot(E,O_{6257}) = e_{6257}$ which shape is (300, 1).\n",
    "Generally $np.dot(E, O_{j}) = e_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start learning some algorithms that can learn word embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to build a language model so it can predict the next word of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/language_model.png\" alt=\"language model\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we use this neural network to learn the language model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/language_model_embedding.png\" alt=\"language model\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get $e_j$ by $np.dot(E,O_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* neural network layer has parameters W1 and b1 while softmax layer has parameters W2 and b2.\n",
    "* Input dimension is (300*6, 1) if the window size is 6 (six previous words).\n",
    "* Here we are optimizing $E$ matrix and layers parameters. We need to maximize the likelihood to predict the next word given the context (previous words).\n",
    "* This model was build in 2003 and tends to work pretty decent for learning word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last example we took a window of 6 words that fall behind the word that we want to predict. There are other choices when we are trying to learn word embeddings.\\\n",
    "Suppose we have an example: \"I want a glass of orange **juice** to go along with my cereal\"\n",
    "To learn juice, choices of context are:\n",
    "1. Last 4 words.\\\n",
    "    We use a window of last 4 words (4 is a hyperparameter), \"a glass of orange\" and try to predict the next word from it.\n",
    "2. 4 words on the left and on the right.\\\n",
    "    \"a glass of orange\" and \"to go along with\"\n",
    "3. Last 1 word.\\\n",
    "    \"orange\"\n",
    "    \n",
    "Researchers found that if you really want to build a language model, it's natural to use the last few words as a context. But if your main goal is really to learn a word embedding, then you can use all of these other contexts and they will result in very meaningful work embeddings as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec creates (context, target) pairs from the text corpus (skip gram) and try to predict the target word from the cotext word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/word_2_vec_model.png\" alt=\"word2vec\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Context word chosen within a window from the target word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/skip_gram.png\" alt=\"skip gram\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe is another algorithm for learning the word embedding. It's very simple algorithm.\n",
    "\n",
    "This is not used as much as word2vec or skip-gram models, but it has some enthusiasts because of its simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more information:https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
