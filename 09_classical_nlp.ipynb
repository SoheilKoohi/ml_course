{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09_classical_nlp.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNTF1yyahM2G8qX1ySiSBPA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhXQfLJAezF5"
      },
      "source": [
        "# Classical Natural Language Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuKtJwgHd6fx"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3pHr1AMoQBI",
        "outputId": "2b7e34c2-b845-4f10-ace7-1553534a0556"
      },
      "source": [
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx9ZFCM5oWem",
        "outputId": "3123701f-528e-4130-b8be-be9c6b3cee6b"
      },
      "source": [
        "!ls /root/nltk_data/tokenizers/punkt"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "czech.pickle\t finnish.pickle  norwegian.pickle   russian.pickle\n",
            "danish.pickle\t french.pickle\t polish.pickle\t    slovene.pickle\n",
            "dutch.pickle\t german.pickle\t portuguese.pickle  spanish.pickle\n",
            "english.pickle\t greek.pickle\t PY3\t\t    swedish.pickle\n",
            "estonian.pickle  italian.pickle  README\t\t    turkish.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waNWf85FokAy"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jPR8c5grg4c"
      },
      "source": [
        "text = \"Helllo! I'm Soheil. I am trying to learn NLP.\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXW15LHorsjb",
        "outputId": "b738f4c9-f72d-4660-eb7b-7661942b8798"
      },
      "source": [
        "sent_tokenize(text)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Helllo!', \"I'm Soheil.\", 'I am trying to learn NLP.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cZY5Xt5r10p"
      },
      "source": [
        "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkBMMoyFsAsC",
        "outputId": "d5621f5a-5e70-4076-9f65-18dbe8f9ae0b"
      },
      "source": [
        "tokenizer.tokenize(text)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Helllo!', \"I'm Soheil.\", 'I am trying to learn NLP.']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-W5ZyIssGUU",
        "outputId": "1e13278a-7c6c-4cb6-ac05-9b67981625da"
      },
      "source": [
        "nltk.download(\"webtext\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaJzPbBast4V"
      },
      "source": [
        "from nltk.corpus import webtext\n",
        "text = webtext.raw(\"overheard.txt\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpfXSYNytBrA"
      },
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "new_tokenizer = PunktSentenceTokenizer(text)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0BiPgXztFqC"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2skEQoNvtib",
        "outputId": "b4f73b22-8ce4-41d9-c6df-ce7858bd0b85"
      },
      "source": [
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(\"Hi, I am Soheil\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi', ',', 'I', 'am', 'Soheil']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx6rH53yv2kV",
        "outputId": "3792a6a0-2cb3-47d8-b500-4b5e185fd957"
      },
      "source": [
        "tokenizer.tokenize(\"سلام، من سهیل هستم.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['سلام،', 'من', 'سهیل', 'هستم', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slTNjjQXv9Ts",
        "outputId": "9744d0f0-ed2f-4c3a-9c73-86e60ee6cc78"
      },
      "source": [
        "tokenizer.tokenize(\"can't\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ca', \"n't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXamxOWWwCtu",
        "outputId": "55755ea2-c2bd-4954-b947-d768beb54ba3"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer_wordpunkt = WordPunctTokenizer()\n",
        "tokenizer_wordpunkt.tokenize(\"can't\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['can', \"'\", 't']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VJwXyEawSSz",
        "outputId": "5244d752-e55f-4fdd-fdbb-4bc525a50d15"
      },
      "source": [
        "print(\"RegEX\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RegEX\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmOXA8YG0XHJ",
        "outputId": "7bde0224-e591-4307-8233-82474b5bfb5e"
      },
      "source": [
        "print(\"remove special characters\")\n",
        "print(\"reove redundunt words\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remove special characters\n",
            "reove redundunt words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuHBXiAX20dq"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nJr9r9V528Uz",
        "outputId": "16edbba4-542f-4e8a-b5f8-f29f445f0ee2"
      },
      "source": [
        "stemmer.stem(\"teaching\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'teach'"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d_OQT6QP2-ov",
        "outputId": "a9dd74f0-f142-462e-877f-13bb6f13f468"
      },
      "source": [
        "stemmer.stem(\"relational\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'relat'"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzvrG7C83UIK"
      },
      "source": [
        "from nltk.stem import LancasterStemmer"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbT0Ky7N3fg3"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"spanish\")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K95da2i37Jl",
        "outputId": "8183d499-27b2-4c13-e40e-6c061b5103b3"
      },
      "source": [
        "SnowballStemmer.languages"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('arabic',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'hungarian',\n",
              " 'italian',\n",
              " 'norwegian',\n",
              " 'porter',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'spanish',\n",
              " 'swedish')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcLR4Y9t3-YW",
        "outputId": "e86ad310-4592-40a3-cea1-18dd9415f61f"
      },
      "source": [
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9mCH_PX4tAg"
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqY00tom4z0Z",
        "outputId": "66a53d40-ff0c-4d20-9c0a-09be85d424ad"
      },
      "source": [
        "stopwords.words(\"english\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iMH6URh43Bg",
        "outputId": "f72e5b4f-76dd-4f0c-8f2f-d322e7199de3"
      },
      "source": [
        "stopwords.fileids()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['arabic',\n",
              " 'azerbaijani',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'greek',\n",
              " 'hungarian',\n",
              " 'indonesian',\n",
              " 'italian',\n",
              " 'kazakh',\n",
              " 'nepali',\n",
              " 'norwegian',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'slovene',\n",
              " 'spanish',\n",
              " 'swedish',\n",
              " 'tajik',\n",
              " 'turkish']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93tkxw5J49Et"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "yYhvEOGp8iJ2",
        "outputId": "2bc036ff-0e99-4bbf-d71c-35ff12e849b9"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer.lemmatize(\"cooking\", pos=\"v\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cook'"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "k5QfaLl48vPQ",
        "outputId": "41b896ba-4a6a-432c-9d4d-3f26f269f993"
      },
      "source": [
        "!pip install hazm"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 31.9 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40 kB 35.1 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 51 kB 37.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 61 kB 37.9 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 71 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 81 kB 28.4 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 92 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 102 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 112 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 122 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 133 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 143 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 153 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 163 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 174 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 184 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 194 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 204 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 215 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 225 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 235 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 245 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 256 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 266 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 276 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 286 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 296 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 307 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 316 kB 32.0 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 41.0 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 43.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394484 sha256=3a8b2c7abd7b15dfc4769537f91f5eeb5bf295e027f4d44c65fbbc3febc23bad\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=153885 sha256=64354d84b21a8b44ff5763aa731befede5d1a912b71602e0d955e018e81a2304\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJBfIpeS-BRc",
        "outputId": "b86a3de5-c67d-433c-a6b9-0ffd6170a2de"
      },
      "source": [
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "\n",
        "normalizer = Normalizer()\n",
        "print(normalizer.normalize('.اصلاح نويسه ها و استفاده از نیم‌فاصله پردازش را آسان مي كند'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ". اصلاح نویسه‌ها و استفاده از نیم‌فاصله پردازش را آسان می‌کند\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIIgGs7uBk-1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}